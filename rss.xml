<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Yuhan Wang]]></title><description><![CDATA[Yuhan Wang is an undergraduate student in computer science.]]></description><link>https://zxh.io</link><generator>RSS for Node</generator><lastBuildDate>Thu, 04 Aug 2022 12:17:45 GMT</lastBuildDate><atom:link href="https://zxh.io/rss.xml" rel="self" type="application/rss+xml"/><copyright><![CDATA[Renovamen 2018-2022]]></copyright><language><![CDATA[en]]></language><item><title><![CDATA[Group Theory]]></title><link>https://zxh.io/post/2022/05/16/hello-world/</link><guid isPermaLink="true">https://zxh.io/post/2022/05/16/hello-world/</guid><dc:creator><![CDATA[Yuhan Wang]]></dc:creator><pubDate>Mon, 16 May 2022 00:00:00 GMT</pubDate></item><item><title><![CDATA[Attention / Conv 大锅烩]]></title><description><![CDATA[<p>长期记录和实现看过的各种论文里的自注意力和卷积机制，咕咕咕，实现地址在：<a href="https://github.com/Renovamen/torchop" target="_blank" rel="noopener noreferrer"><v-icon name="ri-link-m" scale="0.9"/> Github</a></p>
]]></description><link>https://zxh.io/post/2021/08/31/attention-conv/</link><guid isPermaLink="true">https://zxh.io/post/2021/08/31/attention-conv/</guid><dc:creator><![CDATA[Renovamen]]></dc:creator><pubDate>Tue, 31 Aug 2021 00:00:00 GMT</pubDate></item><item><title><![CDATA[自然梯度]]></title><description><![CDATA[<p>自然梯度下降（Natural Gradient Decent）把参数看成一种概率分布，然后使用 KL 散度而不是欧氏距离来作为距离的度量，从而更好地描述更新后的分布和原分布有多大的不同。</p>
]]></description><link>https://zxh.io/post/2021/07/28/natural-gradient/</link><guid isPermaLink="true">https://zxh.io/post/2021/07/28/natural-gradient/</guid><dc:creator><![CDATA[Renovamen]]></dc:creator><pubDate>Wed, 28 Jul 2021 00:00:00 GMT</pubDate></item></channel></rss>